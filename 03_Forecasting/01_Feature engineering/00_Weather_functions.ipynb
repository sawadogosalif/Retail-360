{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f279c24-0075-4cc6-9682-bf39248ef9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "# Standard libraries\n",
    "from datetime import datetime\n",
    "from functools import reduce\n",
    " \n",
    "# Thrid-party libraries\n",
    "from ftplib import FTP\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f1efb-8647-43d6-8b92-dfde6d8452e9",
   "metadata": {},
   "source": [
    "# CONNEXION au site :\n",
    "Data informations :\n",
    "https://www.ncei.noaa.gov/\n",
    "https://www.ncei.noaa.gov/products/land-based-station/global-historical-climatology-network-daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65d34c86-e49b-47a9-ae05-d94d74c9c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ncdc_ftp_con(): \n",
    "  \"\"\"\n",
    "    A function that establishes ftp connection via the ftplib module.\n",
    "    \n",
    "    Arguments:\n",
    "        \n",
    "    Returns:\n",
    "      ftplib.FTP \n",
    "  \"\"\"\n",
    "  \n",
    "  # establish ftp connection\n",
    "  # login using no credentials as this ftp is open source\n",
    "  ftp = FTP('ftp.ncdc.noaa.gov')\n",
    "  ftp.login('','')\n",
    "  \n",
    "  # return the ftp object\n",
    "  return ftp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44c8a6-465a-4ae3-9351-baab5651271f",
   "metadata": {},
   "source": [
    "# Copier la list des dépots :\n",
    "\n",
    "https://www.ncei.noaa.gov/pub/data/ghcn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22787659-f643-41cb-ac88-b56dc498eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_from_ftp(ftp: FTP, file_to_download: str, file_store: str):\n",
    "  \"\"\"\n",
    "    A function that copies file from ftp to a predifened location.\n",
    "    \n",
    "    Arguments:\n",
    "        ftp - ftplib.FTP connection\n",
    "        file_to_download - existing file path to ftp\n",
    "        file_store - path file to store the results\n",
    "    \n",
    "    Returns:      \n",
    "  \"\"\"\n",
    "  \n",
    "  # try to download any given file\n",
    "  # report on success and failure correspondingly\n",
    "  try:\n",
    "    ftp.retrbinary(\"RETR \" + file_to_download, open(file_store, 'wb').write)\n",
    "    print(\"Succsessfully downloaded file: \" + file_to_download + \" into: \" + file_store)\n",
    "  except:\n",
    "    print(\"Unsuccessfull download!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537aa8d2-1c99-4fe5-8be5-7cbc1a61debc",
   "metadata": {},
   "source": [
    "# Downloads weather data for speacific year range perio\n",
    "\n",
    "https://www.ncei.noaa.gov/pub/data/ghcn/daily/by_year/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3d04b4-cbd6-4ceb-8ddc-be2dc6d8f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder=weather_by_year_local_path = \"C:/Users/u32118508/OneDrive - UPEC/Telechargement/Advance Analytics/data_acquisition/data_acquisition/weather/\"\n",
    "\n",
    "def get_weather_by_year(ftp: FTP, from_year: int, to_year: int, by_year_ftp_path: str = \"pub/data/ghcn/daily/by_year/\", local_path: str = folder) :\n",
    "  \"\"\"\n",
    "    A function that downloads weather data for speacific year range period.\n",
    "    \n",
    "    Arguments:\n",
    "        ftp - ftplib.FTP connection\n",
    "        from_year  - int, 4 digit integer representing start period\n",
    "        to_year - int, 4 digit integer representing end period\n",
    "        by_year_ftp_path - path where files are stored on the ftp\n",
    "        local_path - path where files are to be written locally\n",
    "    \n",
    "    Returns:\n",
    "  \"\"\"\n",
    "  \n",
    "  # get filenames on the server\n",
    "  # specify location where files should be stored\n",
    "  files_to_download = [by_year_ftp_path + str(x) + \".csv.gz\" for x in range(from_year, to_year + 1, 1)]\n",
    "  files_to_store = [local_path + str(x) + \".csv.gz\" for x in range(from_year, to_year + 1, 1)]\n",
    "  \n",
    "  # download and store all files\n",
    "  for file_to_download, file_to_store in zip(files_to_download, files_to_store):\n",
    "    get_file_from_ftp(ftp, file_to_download, file_to_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726ccb9-eae3-4c0f-8890-f199cae0379f",
   "metadata": {},
   "source": [
    "# Importer les données des stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329a1b83-fa8a-4282-aad5-c8a1b5958d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_station_data(path_file: str):\n",
    "  \"\"\"\n",
    "    A function that imports the weather station data.\n",
    "    Mainly used for obtaining geolocation of the different stations.\n",
    "    \n",
    "    Arguments:\n",
    "        path_file - location where the raw downloaded file is stored.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame like object wit hstation meta data.\n",
    "  \"\"\"\n",
    "  \n",
    "  # since the file are fixed width we need to specify format (read instructions)\n",
    "  # first headings, name of variables\n",
    "  # second start and stop position for each varaible\n",
    "  headings = [\"ID\", \"LATITUDE\", \"LONGITUDE\", \"ELEVATION\", \"STATE\", \"NAME\", \"GSN FLAG\", \"HCN/CRN FLAG\", \"WMO ID\"]\n",
    "  colspecs = [(0, 11), (13, 20), (22, 30), (32, 37), (39, 40), (42, 71), (73, 75), (77, 79), (81, 85)]\n",
    "  \n",
    "  stations_data = pd.read_fwf(path_file, names=headings, header=None, colspecs=colspecs)\n",
    "  \n",
    "  # returning only merge ID and geolocation, as this is all that is needed from this source of data\n",
    "  return stations_data.filter([\"ID\", \"LATITUDE\", \"LONGITUDE\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7f3b13-3cb9-4e69-b585-64a1120c3cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_import_yearly(year: int, weather_files_dir: str = folder):\n",
    "  \"\"\"\n",
    "    A function that imports a single weather dataset (for one year).\n",
    "    \n",
    "    Arguments:\n",
    "      year - integer specifying the year for which weather data to be imported\n",
    "      weather_files_dir - the directory containing the downloaded yearly weather data archives\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame like object with the weather data for a specific year from all stations worldwide\n",
    "  \"\"\"\n",
    " \n",
    "  # predifinge colnames as not in file\n",
    "  # then read file and return only the 4 columns needed\n",
    "  colnames = ['ID', 'YEAR_MONTH_DAY', 'ELEMENT', 'DATA_VALUE', 'M_FLAG', 'Q_FLAG', 'S_FLAG', 'OBS_TIME']\n",
    "  wiy = pd.read_csv(weather_files_dir + str(year) + \".csv.gz\", compression='gzip', names=colnames, low_memory=False) #\n",
    "  \n",
    "  return wiy.filter(['ID', 'YEAR_MONTH_DAY', 'ELEMENT', 'DATA_VALUE'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f11eab-1aa5-488c-a401-29bb464d1a04",
   "metadata": {},
   "source": [
    "# Filtre par pays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bf1571c-d60b-41a7-aa3c-34b17db34e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_select_country(weather_df: pd.DataFrame, country: str):\n",
    "  \"\"\"\n",
    "    A function that filters weather dataframe by country.\n",
    "    \n",
    "    Arguments:\n",
    "        weather_df - raw imported weather data for any given year\n",
    "        country - two letter country code, \"MO\" for Morocco\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame like object filtered by country\n",
    "  \"\"\"\n",
    "  \n",
    "  return weather_df.loc[lambda x: x['ID'].str.slice(stop=2) == country]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1da491-2370-4012-9452-a9d5b6bf14e6",
   "metadata": {},
   "source": [
    "# Traitement de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8173ed-8542-4442-a2dc-08751913e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_dp_flow(years: list, country: int, station_data_file: str = folder+\"/ghcnd-stations.txt\", weather_files_dir: str = folder):\n",
    "  \"\"\"\n",
    "    This function aims to outline the weather dataprep process\n",
    "    \n",
    "    Arguments:\n",
    "        years - years for which data would be prepped in the final prepped weather dataset\n",
    "        country - two letter code of country, \"MO\" for Morocco\n",
    "        station_data_file - the location of the downloaded file which contains the weather stations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame like object containing pivoted information where each row\n",
    "        is unique by station and date of measurement taken, \n",
    "        containing only stations within selected countries\n",
    "  \"\"\"\n",
    "  \n",
    "  # map import and filter weather data function for all years' dataframes\n",
    "  # and then reduce with concat function\n",
    "    #select country (depend depays)\n",
    "  weather_dfs = map(lambda x: weather_select_country(weather_import_yearly(x, weather_files_dir), country), years)\n",
    "  weather_dfs = reduce(lambda x, y: pd.concat([x, y]), weather_dfs)\n",
    "  # 1) take entire weather data\n",
    "  # 2) pivot weather variables\n",
    "  # 3) merge stations' geolocation\n",
    "  return \\\n",
    "    weather_dfs. \\\n",
    "    pivot_table(index=['ID', 'YEAR_MONTH_DAY'], columns='ELEMENT', values='DATA_VALUE'). \\\n",
    "    reset_index(). \\\n",
    "    merge(import_station_data(station_data_file), on = [\"ID\"], how = \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955b2d52-ee9b-422a-a450-3d82c4fbdb32",
   "metadata": {},
   "source": [
    "# Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4606e0f0-a2d8-4e5b-bfa5-4b34e4da9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_set(data):\n",
    "  \"\"\"\n",
    "      A function to produce custom dataset descriptives (may be further expanded)\n",
    " \n",
    "      Arguments: \n",
    "      data - DataFrame, datset to run descriptives for\n",
    " \n",
    "      Returns: DataFrame with descriptive statitics for an input set\n",
    "  \"\"\"\n",
    "  \n",
    "  # 1) Obtain percentage of nas\n",
    "  nas = data.isnull().sum() / data.shape[0] * 100.00\n",
    "  \n",
    "  # 2) Construct df with additional descriptive stats\n",
    "  output = pd.DataFrame({'Variable': data.columns,\n",
    "                         'Missing_Perc': nas}). \\\n",
    "  assign(\n",
    "         Missing_count=data.isnull().sum(),\n",
    "         N_Uniques=data.nunique(),\n",
    "         Total_Obs=data.shape[0],\n",
    "         Type=data.dtypes,\n",
    "         Min=data.min(numeric_only=True),\n",
    "         Median=data.median(numeric_only=True),\n",
    "         Mean=data.mean(numeric_only=True),\n",
    "         Max=data.max(numeric_only=True),\n",
    "         STD=data.std(numeric_only=True)\n",
    "        ) . \\\n",
    "  sort_values(by=['Type', 'Variable']) \n",
    "  \n",
    "  # 3) Clean up\n",
    "  output = output[['Variable', 'Type', 'Total_Obs', 'N_Uniques', 'Missing_Perc', 'Missing_count', 'Min', 'Median', 'Mean', 'Max', 'STD']]\n",
    "  output.reset_index(drop=True, inplace=True)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
